{"cells":[{"cell_type":"markdown","source":["Some initial imports, do **NOT** write any new code here:"],"metadata":{"id":"BHCvjx7tlmo4"},"id":"BHCvjx7tlmo4"},{"cell_type":"code","execution_count":null,"id":"f8bbc7bc","metadata":{"id":"f8bbc7bc"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","from datasets import load_dataset\n","from huggingface_hub import hf_hub_download"]},{"cell_type":"markdown","id":"9a1a2216","metadata":{"id":"9a1a2216"},"source":["# Question 1: MRI Modality Translation (T1 → T2)\n","\n","Magnetic Resonance Imaging (MRI) is a key modality for brain and brain tumour assessment. Different MRI contrasts highlight different tissue properties: for example, T1-weighted images provide good anatomical detail, while T2-weighted images are more sensitive to fluid and oedema, and often make tumour-related abnormalities more conspicuous. In clinical practice, not all contrasts are always acquired for every patient or every time point, and multi-contrast data can be incomplete or heterogeneous. Learning mappings between MRI modalities is therefore relevant for tasks such as image completion, protocol harmonisation, and data augmentation.\n","\n","In this question you will work with a pre-processed dataset derived from a public brain tumour MRI collection. For each patient, paired **T1** and **T2** images have been extracted as **2D slices** from 3D volumes in several canonical views (axial, sagittal, coronal). The data have been co-registered, skull-stripped, and resampled to **single-channel 64×64** slices and split into **training** and **validation** sets. Each example includes:\n","- `patient_id`: an anonymised identifier for the patient  \n","- `split`: the dataset split (`train` or `validation`)  \n","- `view`: the anatomical view (`axial`, `sagittal`, or `coronal`)  \n","- `t1`: a 2D T1-weighted slice (shape `(1, 64, 64)`)  \n","- `t2`: the corresponding T2-weighted slice (same shape as `t1`)\n","\n","Throughout this question, you will treat **T1 slices as inputs** and **T2 slices as targets** in an image-to-image regression setting.\n","\n","This question is divided into three parts:\n","\n","- **Part 1.A – Exploratory Data Analysis and Pre-processing**: you will inspect the dataset, develop simple visualisations, and design a basic intensity normalisation pipeline.  \n","- **Part 1.B – Modality Translation Model**: you will build and train a deep learning model that maps T1 slices to T2 slices, and evaluate its performance quantitatively and qualitatively.  \n","- **Part 1.C – Discussion**: you will critically reflect on your modelling choices, results, and limitations in a concise written analysis.\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","The next few blocks of code provided in the notebook will download and load the dataset from the Hugging Face Hub, and show you basic usage of the `datasets` API for accessing slices and metadata.\n"]},{"cell_type":"code","execution_count":null,"id":"Ao4nsuw25Lki","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ao4nsuw25Lki","outputId":"7f0cfc53-759f-4fe6-da76-abcf8fc12f93"},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['patient_id', 'split', 'view', 't1', 't2'],\n","     num_rows: 116235\n"," }),\n"," Dataset({\n","     features: ['patient_id', 'split', 'view', 't1', 't2'],\n","     num_rows: 39375\n"," }))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = load_dataset(\"dpelacani/mri-t1-t2-2D-sliced-64\", split=\"train\")\n","valid_dataset = load_dataset(\"dpelacani/mri-t1-t2-2D-sliced-64\", split=\"validation\")\n","\n","train_dataset, valid_dataset"]},{"cell_type":"code","execution_count":null,"id":"pdJcGu1f6_6W","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdJcGu1f6_6W","outputId":"35f8fb07-bd1f-4129-9e7f-8f8c45d3c3a7"},"outputs":[{"data":{"text/plain":["(torch.Size([1, 64, 64]), torch.Size([1, 64, 64]))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Applying a transform to the dataset, this can also incluse torchvision transforms\n","def basic_transform_fn(sample_):\n","    if \"t1\" in sample_:\n","        sample_[\"t1\"] = torch.tensor(sample_[\"t1\"])\n","    if \"t2\" in sample_:\n","        sample_[\"t2\"] = torch.tensor(sample_[\"t2\"])\n","    return sample_\n","\n","train_dataset = train_dataset.with_transform(basic_transform_fn)\n","valid_dataset = valid_dataset.with_transform(basic_transform_fn)\n","\n","train_dataset[0][\"t1\"].shape, valid_dataset[0][\"t1\"].shape\n"]},{"cell_type":"markdown","id":"7f672f61","metadata":{"id":"7f672f61"},"source":["<br><br>\n","\n","## 1.A) Exploratory Data Analysis and Pre-processing\n","\n","In this part you will carry out basic exploratory analysis of the paired T1/T2 slice dataset and design a simple pre-processing strategy that will be reused in Part 1.B.\n","\n","Using the provided loading code and any helper functions you choose to implement, you should:\n","\n","1. **Visualise representative T1–T2 pairs**  \n","   - For each anatomical view, plot a small grid of 8 T1/T2 pairs from the training set. Arrange the plots so that for each example the T1 input and its corresponding T2 target can be compared side-by-side (or in successive rows), and ensure that axes and titles are clearly labelled.\n","\n","2. **Analyse intensity distributions**  \n","   - Plot histograms (or density plots) of pixel intensities for T1 and T2 slices, separately for the training and validation sets.  \n","\n","3. **Design a simple intensity pre-processing transform**  \n","   - Based on your histograms of (2), propose and implement a basic normalisation scheme for `t1` and `t2` that is suitable for the task you will be carrying out in Part 1.B. For instance, you might try scaling the pixels to a fixed range such as \\([0,1]\\) or \\([-1,1]\\)) if you feel that is appropriate.\n","\n","   - Apply this transform to the dataset and re-plot a few T1/T2 pairs and intensity histograms to verify that the transformed images are numerically well-behaved while preserving relevant anatomical structure.  \n","\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","Write your answer to this question below. You can use as many code and text blocks as needed.\n","\n","Please, **make sure to thoroughly describe and comment every piece of code that you include in your answer**. You will be expected to understand every line of code that you write.\n","\n","<br>"]},{"cell_type":"markdown","source":["1. **Visualise representative T1–T2 pairs**"],"metadata":{"id":"epIGJPHkKOqo"},"id":"epIGJPHkKOqo"},{"cell_type":"code","execution_count":null,"id":"dd0c4085","metadata":{"id":"dd0c4085"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["<br>\n","\n","2. **Analyse intensity distributions**  "],"metadata":{"id":"y7DAcT66KQpd"},"id":"y7DAcT66KQpd"},{"cell_type":"code","source":[],"metadata":{"id":"36lkfjxoKVYG"},"id":"36lkfjxoKVYG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","3. **Design a simple intensity pre-processing transform**"],"metadata":{"id":"f9zH85uKKZMx"},"id":"f9zH85uKKZMx"},{"cell_type":"code","source":[],"metadata":{"id":"1jcy6l-hKaZl"},"id":"1jcy6l-hKaZl","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"00fc5918","metadata":{"id":"00fc5918"},"source":["<br><br>\n","\n","## 1.B) Modality Translation (T1 → T2)\n","\n","In this part you will build an **image-to-image model** that takes a T1-weighted slice as input and predicts the corresponding T2-weighted slice. Each training example is therefore a pair $(\\text{T1}, \\text{T2})$ with matching spatial dimensions.\n","\n","You are free to design the model architecture, but it must satisfy the following requirements:\n","\n","- It should operate on 2D slices and produce an output with the **same spatial resolution** as the input\n","- It should take `t1` as input and learn to approximate `t2` as output, using the pre-processed data from Part 1.A.  \n","\n","In designing your solution, you must determine:\n","\n","- a **suitable architecture**, keeping in mind that simpler models should remain functional and that more complex designs may yield improved performance;  \n","- the **loss function** and **optimisation setup** you believe appropriate for this task;  \n","- the **evaluation strategy**, including at least one quantitative metric and the qualitative visualisation of predicted T2 images.\n","\n","Key points to consider:\n","\n","- You are expected to design the full solution **yourself**: the model, the preprocessing pipeline, and the overall training procedure. You must also decide on **suitable evaluation metrics** for this problem. **Strong solutions will ensure that the model can both preserve image quality and generalise well to the validation set.**\n","\n","- Only include code that is necessary to reproduce your results. Notebook organisation and clarity form part of the assessment (see the main *README*).\n","\n","- You should ensure that the notebook clearly displays example outputs of your **final trained model on the validation set**, as well as at least one plot showing how your chosen **evaluation metric(s) evolve over training iterations or epochs.**\n","\n","- You may include **up to two** solutions in this notebook, in recognition that partially working attempts may still receive marks under the criteria outlined in the *README*, but you are encouraged to start with a simple, reliable baseline before attempting a more complex model.\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","Write your answer to this question below. You can use as many code and text blocks as needed.\n","\n","Please, **make sure to thoroughly describe and comment every piece of code that you include in your answer**. You will be expected to understand every line of code that you write.\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"id":"e89d41d1","metadata":{"id":"e89d41d1"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8b7b78d3","metadata":{"id":"8b7b78d3"},"source":["<br><br>\n","\n","## 1.C) Discussion (max 250 words)\n","\n","In this part you should provide a concise written discussion (maximum **250 words**) reflecting on your solution to Part 1.B. Your discussion should address the following points:\n","\n","1. **Modelling choices and pre-processing**  \n","   Explain the main decisions behind your model architecture, loss function, and training strategy, and how they relate to the properties of the T1/T2 translation task.\n","\n","2. **Performance and evaluation**  \n","   Interpret your **quantitative** results and **qualitative** reconstructions. Comment on how well your model appears to reconstruct `t2` features.\n","\n","3. **Limitations and possible improvements**  \n","   Identify the main limitations of your current approach and outline concrete avenues for improvement. You may refer to alternative architectures, loss functions, or improved training and validation procedures.\n","\n","Your answer should be technically focused, directly address these points, and remain within the stated word limit. Overly long or off-topic discussions will be penalised.\n","\n","\n","<br>\n","\n","---\n","\n","<br>"]},{"cell_type":"markdown","source":["// Text answer"],"metadata":{"id":"peHM2U_xkNSz"},"id":"peHM2U_xkNSz"},{"cell_type":"markdown","id":"0502d318","metadata":{"id":"0502d318"},"source":["<br>\n","\n","---\n","\n","<br>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":5}